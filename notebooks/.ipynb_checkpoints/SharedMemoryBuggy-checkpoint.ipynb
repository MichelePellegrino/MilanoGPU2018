{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import IPythonMagic\n",
    "from Timer import Timer\n",
    "import pycuda.driver as cuda_driver\n",
    "import pycuda.compiler as cuda_compiler\n",
    "from pycuda.gpuarray import GPUArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python version 3.6.6 (default, Sep 12 2018, 18:26:19) \n",
      "[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]]\n"
     ]
    }
   ],
   "source": [
    "%setup_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registering context in user workspace\n",
      "Creating context\n",
      "PyCUDA version 2018.1.1\n",
      "CUDA version (9, 1, 0)\n",
      "Driver version 10000\n",
      "Using 'Tesla K80' GPU\n",
      " => compute capability: (3, 7)\n",
      " => memory: 11370 / 11441 MB available\n",
      "Created context handle <53346064>\n",
      "Using CUDA cache dir /home/ubuntu/jupyter_notebooks/Michele_Pellegrino/MilanoGPU2018/notebooks/cuda_cache\n"
     ]
    }
   ],
   "source": [
    "%cuda_context_handler context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.6/site-packages/ipykernel_launcher.py:61: UserWarning: The CUDA compiler succeeded, but said the following:\n",
      "kernel.cu(23): warning: variable \"max_shared\" is used before its value is set\n",
      "\n",
      "kernel.cu(9): warning: variable \"gid\" was declared but never referenced\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kernel_src = \"\"\"\n",
    "\n",
    "// OH NOUS! THIS ONE DOES NOT WORK!\n",
    "\n",
    "__global__ void shmemReduction(float* output, float* input, int size, int _nt) {\n",
    "    \n",
    "    // Global memory -> maximum foe every thread\n",
    "    // Block idx is always 0, since we use only one block\n",
    "    \n",
    "    int gid = blockIdx.x * blockIdx.x + threadIdx.x;\n",
    "    float max_value = -9999999.99;\n",
    "    // float max_value = 0.0;\n",
    "    for (int i = threadIdx.x; i<size; i = i + blockDim.x) {\n",
    "        max_value = fmax(max_value, input[i]);\n",
    "    }\n",
    "    \n",
    "    // Check\n",
    "    output[threadIdx.x] = max_value;\n",
    "    \n",
    "    // Find and store the local maximum in shared-memory\n",
    "    int nt = _nt;\n",
    "    __shared__ float* max_shared;\n",
    "    if (threadIdx.x==0) {\n",
    "        *max_shared = nt;\n",
    "    }\n",
    "    __syncthreads();\n",
    "    max_shared[threadIdx.x] = max_value;\n",
    "    \n",
    "    // Sync\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Find the max\n",
    "    /*\n",
    "    if (threadIdx.x<32)\n",
    "        max_shared[threadIdx.x] = fmaxf( max_shared[threadIdx.x], max_shared[threadIdx.x+32] );\n",
    "    if (threadIdx.x<16)\n",
    "        max_shared[threadIdx.x] = fmaxf( max_shared[threadIdx.x], max_shared[threadIdx.x+16] );\n",
    "    if (threadIdx.x<8)\n",
    "        max_shared[threadIdx.x] = fmaxf( max_shared[threadIdx.x], max_shared[threadIdx.x+8] );\n",
    "    if (threadIdx.x<4)\n",
    "        max_shared[threadIdx.x] = fmaxf( max_shared[threadIdx.x], max_shared[threadIdx.x+4] );\n",
    "    if (threadIdx.x<2)\n",
    "        max_shared[threadIdx.x] = fmaxf( max_shared[threadIdx.x], max_shared[threadIdx.x+2] );\n",
    "    if (threadIdx.x<1)\n",
    "        max_shared[threadIdx.x] = fmaxf( max_shared[threadIdx.x], max_shared[threadIdx.x+1] );\n",
    "    */\n",
    "    while ( nt>0 ) {\n",
    "        if (threadIdx.x<nt)\n",
    "            max_shared[threadIdx.x] = fmaxf( max_shared[threadIdx.x], max_shared[threadIdx.x+nt] );\n",
    "        if ( nt>32 )\n",
    "            __syncthreads();\n",
    "        nt = nt/2;\n",
    "    }\n",
    "    \n",
    "    // Write to output\n",
    "    if (threadIdx.x==0)\n",
    "        output[0] = max_shared[0];\n",
    "    \n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "kernel_module = cuda_compiler.SourceModule(kernel_src)\n",
    "kernel_function = kernel_module.get_function(\"shmemReduction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.19247833e-01 2.09652722e-01 3.77405196e-01 9.46727931e-01\n",
      " 7.54551649e-01 3.37600887e-01 8.47209513e-01 1.10091055e-02\n",
      " 1.99576601e-01 5.18888593e-01 9.21331167e-01 9.78999078e-01\n",
      " 5.16635776e-01 9.01036322e-01 2.12440133e-01 1.97986037e-01\n",
      " 5.19733965e-01 7.15089858e-01 8.98318589e-02 3.88168454e-01\n",
      " 5.18138468e-01 8.36922050e-01 3.53600651e-01 4.46089745e-01\n",
      " 6.83903024e-02 1.73523828e-01 1.09058112e-01 2.50061393e-01\n",
      " 1.47166908e-01 9.29702818e-01 4.83856313e-02 6.63865581e-02\n",
      " 6.46059632e-01 9.00658250e-01 1.88357178e-02 3.54131401e-01\n",
      " 7.29642332e-01 6.25991762e-01 3.40741217e-01 6.60854638e-01\n",
      " 1.81026578e-01 1.66939288e-01 6.56038523e-01 4.94452983e-01\n",
      " 9.66727793e-01 8.76094580e-01 6.33156240e-01 6.75236523e-01\n",
      " 6.75791383e-01 5.70015132e-01 4.79828715e-01 2.17579126e-01\n",
      " 1.11496694e-01 1.56065792e-01 5.74997842e-01 8.85223821e-02\n",
      " 4.06107962e-01 9.99683976e-01 2.05004990e-01 9.91612256e-01\n",
      " 7.90132582e-01 1.96737513e-01 3.29242676e-01 1.08444631e-01\n",
      " 1.96626022e-01 5.97724438e-01 3.76562715e-01 6.75305009e-01\n",
      " 9.90684554e-02 6.77227974e-01 1.27557039e-01 9.27575111e-01\n",
      " 2.48705849e-01 5.26016116e-01 7.14069366e-01 6.75575197e-01\n",
      " 3.55372965e-01 8.08944285e-01 2.80226707e-01 9.14878547e-01\n",
      " 4.73517779e-04 4.17675644e-01 2.86317706e-01 6.28339291e-01\n",
      " 3.59821230e-01 9.51744258e-01 8.60733867e-01 8.32195759e-01\n",
      " 7.55798817e-01 5.40522993e-01 4.88604575e-01 6.31109416e-01\n",
      " 8.40739608e-02 3.91407579e-01 6.83657944e-01 3.32675755e-01\n",
      " 3.43259759e-02 9.28755581e-01 8.60092521e-01 4.55998123e-01\n",
      " 9.81680095e-01 1.53493598e-01 8.43090534e-01 6.40163124e-01\n",
      " 5.38253367e-01 2.11892471e-01 1.92899927e-01 3.00399780e-01\n",
      " 6.34201542e-02 8.50740552e-01 4.10445273e-01 3.97317618e-01\n",
      " 1.55639723e-01 9.56010878e-01 6.19324327e-01 4.63179380e-01\n",
      " 7.41833806e-01 3.51609290e-02 4.75655824e-01 1.85990036e-01\n",
      " 9.40909147e-01 7.64107347e-01 4.31325734e-01 1.44670591e-01\n",
      " 6.30215764e-01 8.10990155e-01 7.53284618e-02 2.28552148e-01]\n"
     ]
    }
   ],
   "source": [
    "n = 128\n",
    "a = np.random.rand(n).astype(np.float32)\n",
    "print(a)\n",
    "\n",
    "a_g = GPUArray(a.shape, a.dtype)\n",
    "a_g.set(a)\n",
    "\n",
    "num_threads = 64\n",
    "b = np.empty((1,num_threads)).astype(np.float32)\n",
    "b_g = GPUArray(b.shape, b.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception caught: Resetting to CUDA context context\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3265, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-7-fd1260758ea0>\", line 6, in <module>\n",
      "    b_g.get(b)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/pycuda/gpuarray.py\", line 305, in get\n",
      "    _memcpy_discontig(ary, self, async_=async_, stream=stream)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/pycuda/gpuarray.py\", line 1309, in _memcpy_discontig\n",
      "    drv.memcpy_dtoh(dst, src.gpudata)\n",
      "pycuda._driver.LogicError: cuMemcpyDtoH failed: an illegal memory access was encountered\n",
      "Popping <53346064>\n",
      "Pushing <53346064>\n",
      "==================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mLogicError\u001b[0m                                Traceback (most recent call last)\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result, async_)\u001b[0m\n",
      "\u001b[1;32m   3264\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m-> 3265\u001b[0;31m                     \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_global_ns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m   3266\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m<ipython-input-7-fd1260758ea0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m----> 6\u001b[0;31m \u001b[0mb_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pycuda/gpuarray.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, ary, pagelocked, async_, stream, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    304\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 305\u001b[0;31m             \u001b[0m_memcpy_discontig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masync_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    306\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pycuda/gpuarray.py\u001b[0m in \u001b[0;36m_memcpy_discontig\u001b[0;34m(dst, src, async_, stream)\u001b[0m\n",
      "\u001b[1;32m   1308\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m-> 1309\u001b[0;31m                     \u001b[0mdrv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemcpy_dtoh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpudata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m   1310\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;31mLogicError\u001b[0m: cuMemcpyDtoH failed: an illegal memory access was encountered\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "\u001b[0;32m~/jupyter_notebooks/Michele_Pellegrino/MilanoGPU2018/notebooks/IPythonMagic.py\u001b[0m in \u001b[0;36mcustom_exc\u001b[0;34m(shell, etype, evalue, tb, tb_offset)\u001b[0m\n",
      "\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     72\u001b[0m             \u001b[0;31m# still show the error within the notebook, don't just swallow it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtb_offsetContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     75\u001b[0m         \u001b[0;31m# this registers a custom exception handler for the whole current notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tb_offsetContext' is not defined\n",
      "The original exception:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Custom TB Handler failed, unregistering\n"
     ]
    }
   ],
   "source": [
    "block_size = (num_threads, 1, 1)\n",
    "grid_size = (1,1,1)\n",
    "\n",
    "kernel_function(b_g, a_g, np.int32(n), grid=grid_size, block=block_size)\n",
    "\n",
    "b_g.get(b)\n",
    "print(a)\n",
    "print(b)\n",
    "print(np.max(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
