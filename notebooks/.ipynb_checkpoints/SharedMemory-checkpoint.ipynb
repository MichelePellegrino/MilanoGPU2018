{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import IPythonMagic\n",
    "from Timer import Timer\n",
    "import pycuda.driver as cuda_driver\n",
    "import pycuda.compiler as cuda_compiler\n",
    "from pycuda.gpuarray import GPUArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global logger already initialized!\n"
     ]
    }
   ],
   "source": [
    "%setup_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registering context in user workspace\n",
      "Context already registered! Ignoring\n"
     ]
    }
   ],
   "source": [
    "%cuda_context_handler context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "CompileError",
     "evalue": "nvcc compilation of /tmp/tmpix5pdg7q/kernel.cu failed\n[command: nvcc --cubin -arch sm_37 -I/home/ubuntu/.local/lib/python3.6/site-packages/pycuda/cuda kernel.cu]\n[stderr:\nkernel.cu(21): error: expression must have a constant value\n\nkernel.cu(9): warning: variable \"gid\" was declared but never referenced\n\n1 error detected in the compilation of \"/tmp/tmpxft_00001420_00000000-6_kernel.cpp1.ii\".\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCompileError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-10aad308ef26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m \"\"\"\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mkernel_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcuda_compiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSourceModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_src\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0mkernel_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shmemReduction\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pycuda/compiler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, source, nvcc, options, keep, no_extern_c, arch, code, cache_dir, include_dirs)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         cubin = compile(source, nvcc, options, keep, no_extern_c,\n\u001b[0;32m--> 291\u001b[0;31m                 arch, code, cache_dir, include_dirs)\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpycuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_from_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pycuda/compiler.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(source, nvcc, options, keep, no_extern_c, arch, code, cache_dir, include_dirs, target)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-I\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcompile_plain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnvcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pycuda/compiler.py\u001b[0m in \u001b[0;36mcompile_plain\u001b[0;34m(source, options, keep, nvcc, cache_dir, target)\u001b[0m\n\u001b[1;32m    135\u001b[0m         raise CompileError(\"nvcc compilation of %s failed\" % cu_file_path,\n\u001b[1;32m    136\u001b[0m                 \u001b[0mcmdline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 stderr=stderr.decode(\"utf-8\", \"replace\"))\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstdout\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCompileError\u001b[0m: nvcc compilation of /tmp/tmpix5pdg7q/kernel.cu failed\n[command: nvcc --cubin -arch sm_37 -I/home/ubuntu/.local/lib/python3.6/site-packages/pycuda/cuda kernel.cu]\n[stderr:\nkernel.cu(21): error: expression must have a constant value\n\nkernel.cu(9): warning: variable \"gid\" was declared but never referenced\n\n1 error detected in the compilation of \"/tmp/tmpxft_00001420_00000000-6_kernel.cpp1.ii\".\n]"
     ]
    }
   ],
   "source": [
    "kernel_src = \"\"\"\n",
    "\n",
    "__global__ void shmemReduction(float* output, float* input, int size, int _nt) {\n",
    "    \n",
    "    // Global memory -> maximum foe every thread\n",
    "    // Block idx is always 0, since we use only one block\n",
    "    \n",
    "    int gid = blockIdx.x * blockIdx.x + threadIdx.x;\n",
    "    float max_value = -9999999.99;\n",
    "    // float max_value = 0.0;\n",
    "    for (int i = threadIdx.x; i<size; i = i + blockDim.x) {\n",
    "        max_value = fmax(max_value, input[i]);\n",
    "    }\n",
    "    \n",
    "    // Check\n",
    "    output[threadIdx.x] = max_value;\n",
    "    \n",
    "    // Find and store the local maximum in shared-memory\n",
    "    int nt = _nt;\n",
    "    __shared__ float max_shared[nt];\n",
    "    max_shared[threadIdx.x] = max_value;\n",
    "    \n",
    "    // Sync\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Find the max\n",
    "    /*\n",
    "    if (threadIdx.x<32)\n",
    "        max_shared[threadIdx.x] = fmaxf( max_shared[threadIdx.x], max_shared[threadIdx.x+32] );\n",
    "    if (threadIdx.x<16)\n",
    "        max_shared[threadIdx.x] = fmaxf( max_shared[threadIdx.x], max_shared[threadIdx.x+16] );\n",
    "    if (threadIdx.x<8)\n",
    "        max_shared[threadIdx.x] = fmaxf( max_shared[threadIdx.x], max_shared[threadIdx.x+8] );\n",
    "    if (threadIdx.x<4)\n",
    "        max_shared[threadIdx.x] = fmaxf( max_shared[threadIdx.x], max_shared[threadIdx.x+4] );\n",
    "    if (threadIdx.x<2)\n",
    "        max_shared[threadIdx.x] = fmaxf( max_shared[threadIdx.x], max_shared[threadIdx.x+2] );\n",
    "    if (threadIdx.x<1)\n",
    "        max_shared[threadIdx.x] = fmaxf( max_shared[threadIdx.x], max_shared[threadIdx.x+1] );\n",
    "    */\n",
    "    while ( nt>0 ) {\n",
    "        if (threadIdx.x<nt)\n",
    "            max_shared[threadIdx.x] = fmaxf( max_shared[threadIdx.x], max_shared[threadIdx.x+nt] );\n",
    "        if ( nt>32 )\n",
    "            __syncthreads();\n",
    "        nt = nt/2;\n",
    "    }\n",
    "    \n",
    "    // Write to output\n",
    "    if (threadIdx.x==0)\n",
    "        output[0] = max_shared[0];\n",
    "    \n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "kernel_module = cuda_compiler.SourceModule(kernel_src)\n",
    "kernel_function = kernel_module.get_function(\"shmemReduction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7848292  0.8035179  0.6661633  0.4362775  0.14559805 0.2237722\n",
      " 0.8979147  0.24531668 0.6544041  0.20344712 0.7684155  0.64986336\n",
      " 0.6732486  0.11934242 0.6662393  0.24418062 0.76820046 0.6371303\n",
      " 0.5391755  0.07863975 0.85514474 0.30057132 0.3635121  0.9111666\n",
      " 0.93807524 0.67226166 0.9067363  0.67060715 0.2590795  0.9183377\n",
      " 0.44751853 0.25068313 0.47307548 0.38210386 0.5861356  0.6989991\n",
      " 0.7308389  0.632069   0.1522887  0.5991435  0.2317969  0.13030311\n",
      " 0.98944765 0.9419435  0.8709065  0.01420743 0.02860881 0.683816\n",
      " 0.66840166 0.8669783  0.01202685 0.5459647  0.4260259  0.05549169\n",
      " 0.86324567 0.95831513 0.13250278 0.20227909 0.9709796  0.12757729\n",
      " 0.05081191 0.03339012 0.55195785 0.06025    0.41970348 0.4406396\n",
      " 0.47934917 0.97619104 0.0918419  0.12877244 0.3438261  0.5032521\n",
      " 0.4812024  0.7640697  0.57360566 0.7157317  0.20185104 0.7379211\n",
      " 0.85921705 0.86202437 0.36600062 0.17110272 0.46811852 0.73640066\n",
      " 0.8052173  0.1754387  0.3654525  0.35389277 0.9781277  0.67134196\n",
      " 0.9064665  0.53892994 0.05673488 0.34612715 0.27724576 0.3620857\n",
      " 0.8970666  0.5747186  0.08472421 0.4590253  0.5613736  0.19799665\n",
      " 0.8797027  0.3331014  0.07139413 0.47079578 0.5633346  0.4753453\n",
      " 0.7636181  0.24766064 0.05730652 0.8569291  0.6327542  0.22090265\n",
      " 0.4254008  0.7489153  0.21149363 0.5898998  0.115022   0.9887535\n",
      " 0.07304413 0.3396252  0.20678125 0.29976085 0.12332702 0.00715888\n",
      " 0.49209598 0.27876985]\n"
     ]
    }
   ],
   "source": [
    "n = 128\n",
    "a = np.random.rand(n).astype(np.float32)\n",
    "print(a)\n",
    "\n",
    "a_g = GPUArray(a.shape, a.dtype)\n",
    "a_g.set(a)\n",
    "\n",
    "num_threads = 64\n",
    "b = np.empty((1,num_threads)).astype(np.float32)\n",
    "b_g = GPUArray(b.shape, b.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7848292  0.8035179  0.6661633  0.4362775  0.14559805 0.2237722\n",
      " 0.8979147  0.24531668 0.6544041  0.20344712 0.7684155  0.64986336\n",
      " 0.6732486  0.11934242 0.6662393  0.24418062 0.76820046 0.6371303\n",
      " 0.5391755  0.07863975 0.85514474 0.30057132 0.3635121  0.9111666\n",
      " 0.93807524 0.67226166 0.9067363  0.67060715 0.2590795  0.9183377\n",
      " 0.44751853 0.25068313 0.47307548 0.38210386 0.5861356  0.6989991\n",
      " 0.7308389  0.632069   0.1522887  0.5991435  0.2317969  0.13030311\n",
      " 0.98944765 0.9419435  0.8709065  0.01420743 0.02860881 0.683816\n",
      " 0.66840166 0.8669783  0.01202685 0.5459647  0.4260259  0.05549169\n",
      " 0.86324567 0.95831513 0.13250278 0.20227909 0.9709796  0.12757729\n",
      " 0.05081191 0.03339012 0.55195785 0.06025    0.41970348 0.4406396\n",
      " 0.47934917 0.97619104 0.0918419  0.12877244 0.3438261  0.5032521\n",
      " 0.4812024  0.7640697  0.57360566 0.7157317  0.20185104 0.7379211\n",
      " 0.85921705 0.86202437 0.36600062 0.17110272 0.46811852 0.73640066\n",
      " 0.8052173  0.1754387  0.3654525  0.35389277 0.9781277  0.67134196\n",
      " 0.9064665  0.53892994 0.05673488 0.34612715 0.27724576 0.3620857\n",
      " 0.8970666  0.5747186  0.08472421 0.4590253  0.5613736  0.19799665\n",
      " 0.8797027  0.3331014  0.07139413 0.47079578 0.5633346  0.4753453\n",
      " 0.7636181  0.24766064 0.05730652 0.8569291  0.6327542  0.22090265\n",
      " 0.4254008  0.7489153  0.21149363 0.5898998  0.115022   0.9887535\n",
      " 0.07304413 0.3396252  0.20678125 0.29976085 0.12332702 0.00715888\n",
      " 0.49209598 0.27876985]\n",
      "[[0.97619104 0.8035179  0.6661633  0.97619104 0.14559805 0.2237722\n",
      "  0.8979147  0.5032521  0.6544041  0.7640697  0.7684155  0.7157317\n",
      "  0.6732486  0.7379211  0.85921705 0.86202437 0.76820046 0.6371303\n",
      "  0.5391755  0.73640066 0.85514474 0.30057132 0.3654525  0.9111666\n",
      "  0.9781277  0.67226166 0.9067363  0.67060715 0.2590795  0.9183377\n",
      "  0.44751853 0.3620857  0.8970666  0.5747186  0.5861356  0.6989991\n",
      "  0.7308389  0.632069   0.8797027  0.5991435  0.2317969  0.47079578\n",
      "  0.98944765 0.9419435  0.8709065  0.24766064 0.05730652 0.8569291\n",
      "  0.66840166 0.8669783  0.4254008  0.7489153  0.4260259  0.5898998\n",
      "  0.86324567 0.9887535  0.13250278 0.3396252  0.9709796  0.29976085\n",
      "  0.12332702 0.03339012 0.55195785 0.27876985]]\n",
      "0.98944765\n"
     ]
    }
   ],
   "source": [
    "block_size = (num_threads, 1, 1)\n",
    "grid_size = (1,1,1)\n",
    "\n",
    "kernel_function(b_g, a_g, np.int32(n), grid=grid_size, block=block_size)\n",
    "\n",
    "b_g.get(b)\n",
    "print(a)\n",
    "print(b)\n",
    "print(np.max(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
